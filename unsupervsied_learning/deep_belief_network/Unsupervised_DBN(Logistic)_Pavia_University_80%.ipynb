{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Unsupervised DBN(Logistic) - Pavia University.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6359c9dc"
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "\n",
        "#To load Hyperspectral image cube .mat file as input\n",
        "import scipy\n",
        "from scipy.io import loadmat\n",
        "\n",
        "\n",
        "# To split training and testing data\n",
        "import sklearn as sk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Principal Component Analysis \n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Pipelining\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn import linear_model, metrics\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from scipy.special import expit\n",
        "\n",
        "from sklearn.preprocessing import normalize, MinMaxScaler, OneHotEncoder\n",
        "from sklearn.utils import shuffle\n",
        "from scipy.stats import truncnorm\n",
        "\n",
        "from keras.utils import np_utils\n",
        "\n"
      ],
      "id": "6359c9dc",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbuQ_89otYiM",
        "outputId": "cb0036c3-eff0-4bd6-c1a0-9b0540a2b0e6"
      },
      "source": [
        "!wget http://www.ehu.eus/ccwintco/uploads/e/ee/PaviaU.mat    http://www.ehu.eus/ccwintco/uploads/5/50/PaviaU_gt.mat"
      ],
      "id": "BbuQ_89otYiM",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-04 11:12:29--  http://www.ehu.eus/ccwintco/uploads/e/ee/PaviaU.mat\n",
            "Resolving www.ehu.eus (www.ehu.eus)... 158.227.0.65, 2001:720:1410::65\n",
            "Connecting to www.ehu.eus (www.ehu.eus)|158.227.0.65|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 34806917 (33M)\n",
            "Saving to: ‘PaviaU.mat’\n",
            "\n",
            "PaviaU.mat          100%[===================>]  33.19M   582KB/s    in 60s     \n",
            "\n",
            "2021-06-04 11:13:29 (568 KB/s) - ‘PaviaU.mat’ saved [34806917/34806917]\n",
            "\n",
            "--2021-06-04 11:13:29--  http://www.ehu.eus/ccwintco/uploads/5/50/PaviaU_gt.mat\n",
            "Connecting to www.ehu.eus (www.ehu.eus)|158.227.0.65|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11005 (11K)\n",
            "Saving to: ‘PaviaU_gt.mat’\n",
            "\n",
            "PaviaU_gt.mat       100%[===================>]  10.75K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-06-04 11:13:30 (233 MB/s) - ‘PaviaU_gt.mat’ saved [11005/11005]\n",
            "\n",
            "FINISHED --2021-06-04 11:13:30--\n",
            "Total wall clock time: 1m 1s\n",
            "Downloaded: 2 files, 33M in 1m 0s (568 KB/s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71a0be77"
      },
      "source": [
        "def load_dataset():\n",
        "    \n",
        "    # X is the input dataset\n",
        "    X = loadmat('PaviaU.mat')['paviaU']\n",
        "  \n",
        "    # y is the labelled dataset\n",
        "    y = loadmat('PaviaU_gt.mat')['paviaU_gt']\n",
        "\n",
        "    print(\"Load dataset\\n\")\n",
        "    \n",
        "    X = (X-np.min(X))/(np.max(X)-np.min(X))\n",
        "    \n",
        "    # Print shape of input dataset (X) and labelled dataset (y)\n",
        "    print(f\"X shape : {X.shape}\\ny shape : {y.shape}\\n\\n\\n\")\n",
        "    \n",
        "    return X,y"
      ],
      "id": "71a0be77",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf013a5d"
      },
      "source": [
        "class PreProcessing:\n",
        "    \n",
        "    def __init__(self, X, y, pca_components, window_size):\n",
        "        \n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.pca_components = pca_components\n",
        "        self.window_size = window_size\n",
        "        self.X_spectrum_vector = np.zeros((self.X.shape[0]*self.X.shape[1], self.X.shape[2],))\n",
        "        self.X_pca = np.zeros((self.X.shape[0]*self.X.shape[1], pca_components))\n",
        "        self.X_spatial_vector = np.zeros((self.X.shape[0]*self.X.shape[1], window_size*window_size*pca_components))\n",
        "        \n",
        "    \n",
        "    \n",
        "    def normalize_vectors(self,X):\n",
        "        #return normalize(X)\n",
        "        return (X-np.min(X))/(np.max(X)-np.min(X))\n",
        "    \n",
        "    \n",
        "    def spectrum_vector(self):\n",
        "        \n",
        "        k=0\n",
        "        \n",
        "        for i in range(self.X.shape[0]):\n",
        "            for j in range(self.X.shape[1]):\n",
        "                X_spectrum = self.X[i,j,:]\n",
        "                X_spectrum = X_spectrum.flatten()\n",
        "                self.X_spectrum_vector[k,:] = X_spectrum\n",
        "                k+=1\n",
        "                \n",
        "        #self.normalize_vectors(self.X_spectrum_vector)\n",
        "        \n",
        "        print(f\"Spectrum vector shape : {self.X_spectrum_vector.shape}\")\n",
        "        \n",
        "    \n",
        "    def apply_pca(self):\n",
        "  \n",
        "        print(\"\\n\\nApply PCA\\n\")\n",
        "\n",
        "        # Reshaping X from image dimensions to a list\n",
        "        temp_X = np.reshape(self.X,(-1,self.X.shape[2]))\n",
        "        print(f\"temp_X shape : {temp_X.shape}\")\n",
        "  \n",
        "        # Creating an objcect of class PCA with number of components 100\n",
        "        pca = PCA(n_components = self.pca_components, whiten = True)\n",
        "  \n",
        "        # Applying and transforming dataset by PCA by removing the last column that is labelled column\n",
        "        pca_X = pca.fit_transform(temp_X[:,:])\n",
        "\n",
        "        # Reshaping new_X back to image dimensions with reduced number of bands\n",
        "        self.X_pca = np.reshape(pca_X, (self.X.shape[0], self.X.shape[1], self.pca_components))\n",
        "  \n",
        "        print(f\"X shape after applying PCA dimensionality reduction technique : {self.X_pca.shape}\\n\\n\\n\")\n",
        " \n",
        "    \n",
        "    def zero_padding_X(self, X, margin):\n",
        "\n",
        "        # Creating a numpy 3D matrix of zeros with the required shape for zero padding\n",
        "        zero_padded_X = np.zeros((X.shape[0]+2*margin, X.shape[1]+2*margin, X.shape[2]))\n",
        "\n",
        "        # Offset is the starting address of hyperspectral image within zero_padded_X \n",
        "        offset = margin \n",
        "\n",
        "        # Copying the contents of X into zero_padded_x with correct offset address \n",
        "        zero_padded_X[offset:X.shape[0]+offset, offset:X.shape[1]+offset,:] = X\n",
        "\n",
        "        print(f\"The shape of X after zero padding : {zero_padded_X.shape}\")\n",
        "\n",
        "        return zero_padded_X\n",
        "    \n",
        "    def zero_padding_y(self, y, margin):\n",
        "\n",
        "        # Creating a numpy 2D matrix of zeros with the required shape for zero padding\n",
        "        zero_padded_y = np.zeros((y.shape[0]+2*margin, y.shape[1]+2*margin))\n",
        "\n",
        "        # Offset is the starting address of hyperspectral image within zero_padded_X \n",
        "        offset = margin \n",
        "\n",
        "        # Copying the contents of X into zero_padded_x with correct offset address \n",
        "        zero_padded_y[offset:y.shape[0]+offset, offset:y.shape[1]+offset] = y\n",
        "\n",
        "        print(f\"The shape of y after zero padding : {zero_padded_y.shape}\")\n",
        "        #print(np.unique(zero_padded_y))\n",
        "        return zero_padded_y\n",
        "     \n",
        "    \n",
        "    def spatial_vector(self, labelling_type = \"center\"):\n",
        "\n",
        "        X = self.X_pca\n",
        "        y = self.y\n",
        "        window_size = self.window_size \n",
        "        \n",
        "        print(\"Create smaller image cuboids\\n\")\n",
        "  \n",
        "        stride = 1\n",
        "  \n",
        "        # Calculating the margin required for zero padding for a particular window size (or smaller cube size)\n",
        "        zero_padding_margin = int((window_size - stride) / 2)\n",
        "  \n",
        "        # Applying zero padding to X\n",
        "        zero_padded_X = self.zero_padding_X(X, zero_padding_margin)\n",
        "\n",
        "        # Apply zero padding to y\n",
        "        zero_padded_y = self.zero_padding_y(y, zero_padding_margin)\n",
        "\n",
        "        # Create a matrix for assigning labels to smaller cuboids X_cuboids\n",
        "        y_cuboids_labels = np.zeros((X.shape[0]*X.shape[1]))\n",
        "\n",
        "        offset = zero_padding_margin\n",
        "\n",
        "        # Traversing through X and y to split X and y accordingly\n",
        "        index = 0\n",
        "        for i in range(offset,zero_padded_X.shape[0]-offset):\n",
        "            for j in range(offset, zero_padded_X.shape[1]-offset):\n",
        "\n",
        "                # Extracting smaller cube from zero_padded_X\n",
        "                cube = zero_padded_X[i-offset:i+offset+1,j-offset:j+offset+1]\n",
        "\n",
        "                # Storing the extracted cube from zero_padded_X into X_cuboids\n",
        "                self.X_spatial_vector[index,:] = cube.flatten() \n",
        "\n",
        "                # Storing the extracted matrix from zero_padded_y into y_labels\n",
        "                y_labels = zero_padded_y[i-offset:i+offset+1,j-offset:j+offset+1]\n",
        "        \n",
        "        \n",
        "                if labelling_type==\"majority\":\n",
        "          \n",
        "                    # Calculating mode of y_labels matrix\n",
        "                    y_label = mode(y_labels, axis=None, nan_policy=\"omit\")\n",
        "          \n",
        "                    # Assigning label to a particular index's X_cuboid\n",
        "                    y_cuboids_labels[index] = int(y_label[0])\n",
        "\n",
        "                elif labelling_type==\"center\":\n",
        "\n",
        "                    y_label = y_labels[y_labels.shape[0]//2,y_labels.shape[1]//2]\n",
        "                    y_cuboids_labels[index] = int(y_label)\n",
        "\n",
        "                #print(y_label)                \n",
        "                \n",
        "                index+=1   \n",
        "                  \n",
        "        self.y = y_cuboids_labels.flatten()\n",
        "        \n",
        "        self.normalize_vectors(self.X_spatial_vector)\n",
        "        \n",
        "        #scaler = MinMaxScaler(copy=True)\n",
        "        #caler.fit_transform(X_spatial_vector)\n",
        "\n",
        "        \n",
        "        print(f\"\\n\\nSpatial vector shape : {self.X_spatial_vector.shape}\")\n",
        "        print(f\"The shape of y_cuboids_labels : {y_cuboids_labels.shape}\\n\\n\\n\")\n",
        " \n",
        "    \n",
        "    def remove_class_zero(self):\n",
        "\n",
        "        \n",
        "        print(\"\\n\\nRemoving class zero\\n\")\n",
        "        \n",
        "        self.X_spatial_vector = self.X_spatial_vector[self.y>0,:]\n",
        "        self.X_spectrum_vector = self.X_spectrum_vector[self.y>0,:]\n",
        "        self.y = self.y[self.y>0]\n",
        "        print(f\"Classes available for classification after removing class 0 : {np.unique(self.y)}\")\n",
        "        self.y -= 1 \n",
        "        print(f\"New Class labels starting from 0 :  {np.unique(self.y)}\")\n",
        "            \n",
        "        \n",
        "        print(f\"\\n\\nSpectrum vector shape after removing class zero: {self.X_spectrum_vector.shape}\")\n",
        "        print(f\"\\n\\nSpatial vector shape after removing class zero: {self.X_spatial_vector.shape}\")\n",
        "        print(f\"The shape of y after removing class zero : {self.y.shape}\\n\\n\\n\")\n",
        "\n",
        "        #only spatial vector\n",
        "        self.X = self.X_spatial_vector\n",
        "        self.y = self.y.flatten()\n",
        "        \n",
        "        print(f\"Combined final spactial vector shape : {self.X.shape}\")\n",
        "        print(f\"y shape : {self.y.shape}\")\n",
        " \n",
        "            \n",
        " \n",
        "    \n",
        "    #def combine_vectors(self):\n",
        "        \n",
        "        #self.X = np.hstack((self.X_spectrum_vector, self.X_spatial_vector))\n",
        "        #self.y = self.y.flatten()\n",
        "        \n",
        "        #print(f\"Combined final vector shape : {self.X.shape}\")\n",
        "        #print(f\"y shape : {self.y.shape}\")\n",
        "    \n",
        "    def count_frequency(self, y, name):\n",
        "        \n",
        "        unique, counts = np.unique(y, return_counts=True)\n",
        "        elements=dict(zip(unique, counts))\n",
        "        print(f\"Number of elements present in each class of {name}: {elements}\")      \n",
        "    \n",
        "    def split_dataset(self, training_size = 500):\n",
        "        \n",
        "        \n",
        "        \n",
        "        #one_hot_encoder = OneHotEncoder(sparse=False)\n",
        "        #one_hot_encoder.fit_transform(self.y)\n",
        "        \n",
        "        #self.y = np_utils.to_categorical(self.y)\n",
        "        \n",
        "        print(\"\\n\\nSplitting Dataset\\n\" )\n",
        "        \n",
        "        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y,\n",
        "                                                            train_size=0.6,\n",
        "                                                            random_state=10,\n",
        "                                                            shuffle=True,\n",
        "                                                            stratify=self.y)\n",
        "        self.count_frequency(self.y, name=\"y\")            \n",
        "             \n",
        "      \n",
        "        \n",
        "        # Print shapes of training and testing datasets\n",
        "        print(f\"X_train shape : {X_train.shape}\")\n",
        "        print(f\"y_train shape : {y_train.shape}\")\n",
        "        print(f\"X_test shape  : {X_test.shape}\")\n",
        "        print(f\"y_test shape  : {y_test.shape}\")\n",
        "        \n",
        "        self.count_frequency(y_train, name=\"y_train\")\n",
        "        self.count_frequency(y_test, name=\"y_test\")\n",
        "        \n",
        "        return (X_train, X_test, y_train, y_test)\n",
        "    \n",
        "        \n",
        "        \n",
        "        \n",
        "    "
      ],
      "id": "bf013a5d",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "888e9dcc"
      },
      "source": [
        "class BinaryRBM:\n",
        "    \"\"\"\n",
        "    This class implements a Binary Restricted Boltzmann machine.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 n_hidden_units=100,\n",
        "                 activation_function='sigmoid',\n",
        "                 optimization_algorithm='sgd',\n",
        "                 learning_rate=1e-3,\n",
        "                 n_epochs=10,\n",
        "                 contrastive_divergence_iter=1,\n",
        "                 batch_size=32,\n",
        "                 verbose=True):\n",
        "        self.n_hidden_units = n_hidden_units\n",
        "        self.activation_function = activation_function\n",
        "        self.optimization_algorithm = optimization_algorithm\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_epochs = n_epochs\n",
        "        self.contrastive_divergence_iter = contrastive_divergence_iter\n",
        "        self.batch_size = batch_size\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def fit(self, X):\n",
        "        \"\"\"\n",
        "        Fit a model given data.\n",
        "        :param X: array-like, shape = (n_samples, n_features)\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # Initialize RBM parameters\n",
        "        self.n_visible_units = X.shape[1]\n",
        "        if self.activation_function == 'sigmoid':\n",
        "            self.W = np.random.randn(self.n_hidden_units, self.n_visible_units) / np.sqrt(self.n_visible_units)\n",
        "            self.c = np.random.randn(self.n_hidden_units) / np.sqrt(self.n_visible_units)\n",
        "            self.b = np.random.randn(self.n_visible_units) / np.sqrt(self.n_visible_units)\n",
        "            self._activation_function_class = SigmoidActivationFunction\n",
        "        elif self.activation_function == 'relu':\n",
        "            self.W = truncnorm.rvs(-0.2, 0.2, size=[self.n_hidden_units, self.n_visible_units]) / np.sqrt(\n",
        "                self.n_visible_units)\n",
        "            self.c = np.full(self.n_hidden_units, 0.1) / np.sqrt(self.n_visible_units)\n",
        "            self.b = np.full(self.n_visible_units, 0.1) / np.sqrt(self.n_visible_units)\n",
        "            self._activation_function_class = ReLUActivationFunction\n",
        "        else:\n",
        "            raise ValueError(\"Invalid activation function.\")\n",
        "\n",
        "        if self.optimization_algorithm == 'sgd':\n",
        "            self._stochastic_gradient_descent(X)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid optimization algorithm.\")\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Transforms data using the fitted model.\n",
        "        :param X: array-like, shape = (n_samples, n_features)\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if len(X.shape) == 1:  # It is a single sample\n",
        "            return self._compute_hidden_units(X)\n",
        "        transformed_data = self._compute_hidden_units_matrix(X)\n",
        "        return transformed_data\n",
        "\n",
        "    def _reconstruct(self, transformed_data):\n",
        "        \"\"\"\n",
        "        Reconstruct visible units given the hidden layer output.\n",
        "        :param transformed_data: array-like, shape = (n_samples, n_features)\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        return self._compute_visible_units_matrix(transformed_data)\n",
        "\n",
        "    \n",
        "    def batch_generator(self, batch_size, data, labels=None):\n",
        "        \"\"\"\n",
        "        Generates batches of samples\n",
        "        :param data: array-like, shape = (n_samples, n_features)\n",
        "        :param labels: array-like, shape = (n_samples, )\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        n_batches = int(np.ceil(len(data) / float(batch_size)))\n",
        "        #idx = np.random.permutation(len(data))\n",
        "        data_shuffled = data\n",
        "        if labels is not None:\n",
        "            labels_shuffled = labels[idx]\n",
        "        for i in range(n_batches):\n",
        "            start = i * batch_size\n",
        "            end = start + batch_size\n",
        "            if labels is not None:\n",
        "                yield data_shuffled[start:end, :], labels_shuffled[start:end]\n",
        "            else:\n",
        "                yield data_shuffled[start:end, :]\n",
        "  \n",
        "    def _stochastic_gradient_descent(self, _data):\n",
        "        \"\"\"\n",
        "        Performs stochastic gradient descend optimization algorithm.\n",
        "        :param _data: array-like, shape = (n_samples, n_features)\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        accum_delta_W = np.zeros(self.W.shape)\n",
        "        accum_delta_b = np.zeros(self.b.shape)\n",
        "        accum_delta_c = np.zeros(self.c.shape)\n",
        "        for iteration in range(1, self.n_epochs + 1):\n",
        "            idx = np.random.permutation(len(_data))\n",
        "            data = _data[idx]\n",
        "            for batch in self.batch_generator(self.batch_size, data):\n",
        "                accum_delta_W[:] = .0\n",
        "                accum_delta_b[:] = .0\n",
        "                accum_delta_c[:] = .0\n",
        "                for sample in batch:\n",
        "                    delta_W, delta_b, delta_c = self._contrastive_divergence(sample)\n",
        "                    accum_delta_W += delta_W\n",
        "                    accum_delta_b += delta_b\n",
        "                    accum_delta_c += delta_c\n",
        "                self.W += self.learning_rate * (accum_delta_W / self.batch_size)\n",
        "                self.b += self.learning_rate * (accum_delta_b / self.batch_size)\n",
        "                self.c += self.learning_rate * (accum_delta_c / self.batch_size)\n",
        "            if self.verbose:\n",
        "                error = self._compute_reconstruction_error(data)\n",
        "                print(\">> Epoch %d finished \\tRBM Reconstruction error %f\" % (iteration, error))\n",
        "\n",
        "    def _contrastive_divergence(self, vector_visible_units):\n",
        "        \"\"\"\n",
        "        Computes gradients using Contrastive Divergence method.\n",
        "        :param vector_visible_units: array-like, shape = (n_features, )\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        v_0 = vector_visible_units\n",
        "        v_t = np.array(v_0)\n",
        "\n",
        "        # Sampling\n",
        "        for t in range(self.contrastive_divergence_iter):\n",
        "            h_t = self._sample_hidden_units(v_t)\n",
        "            v_t = self._compute_visible_units(h_t)\n",
        "\n",
        "        # Computing deltas\n",
        "        v_k = v_t\n",
        "        h_0 = self._compute_hidden_units(v_0)\n",
        "        h_k = self._compute_hidden_units(v_k)\n",
        "        delta_W = np.outer(h_0, v_0) - np.outer(h_k, v_k)\n",
        "        delta_b = v_0 - v_k\n",
        "        delta_c = h_0 - h_k\n",
        "\n",
        "        return delta_W, delta_b, delta_c\n",
        "\n",
        "    def _sample_hidden_units(self, vector_visible_units):\n",
        "        \"\"\"\n",
        "        Computes hidden unit activations by sampling from a binomial distribution.\n",
        "        :param vector_visible_units: array-like, shape = (n_features, )\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        hidden_units = self._compute_hidden_units(vector_visible_units)\n",
        "        return (np.random.random_sample(len(hidden_units)) < hidden_units).astype(np.int64)\n",
        "\n",
        "    def _sample_visible_units(self, vector_hidden_units):\n",
        "        \"\"\"\n",
        "        Computes visible unit activations by sampling from a binomial distribution.\n",
        "        :param vector_hidden_units: array-like, shape = (n_features, )\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        visible_units = self._compute_visible_units(vector_hidden_units)\n",
        "        return (np.random.random_sample(len(visible_units)) < visible_units).astype(np.int64)\n",
        "\n",
        "    def _compute_hidden_units(self, vector_visible_units):\n",
        "        \"\"\"\n",
        "        Computes hidden unit outputs.\n",
        "        :param vector_visible_units: array-like, shape = (n_features, )\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        v = np.expand_dims(vector_visible_units, 0)\n",
        "        h = np.squeeze(self._compute_hidden_units_matrix(v))\n",
        "        return np.array([h]) if not h.shape else h\n",
        "\n",
        "    def _compute_hidden_units_matrix(self, matrix_visible_units):\n",
        "        \"\"\"\n",
        "        Computes hidden unit outputs.\n",
        "        :param matrix_visible_units: array-like, shape = (n_samples, n_features)\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        return np.transpose(self._activation_function_class.function(\n",
        "            np.dot(self.W, np.transpose(matrix_visible_units)) + self.c[:, np.newaxis]))\n",
        "\n",
        "    def _compute_visible_units(self, vector_hidden_units):\n",
        "        \"\"\"\n",
        "        Computes visible (or input) unit outputs.\n",
        "        :param vector_hidden_units: array-like, shape = (n_features, )\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        h = np.expand_dims(vector_hidden_units, 0)\n",
        "        v = np.squeeze(self._compute_visible_units_matrix(h))\n",
        "        return np.array([v]) if not v.shape else v\n",
        "\n",
        "    def _compute_visible_units_matrix(self, matrix_hidden_units):\n",
        "        \"\"\"\n",
        "        Computes visible (or input) unit outputs.\n",
        "        :param matrix_hidden_units: array-like, shape = (n_samples, n_features)\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        return self._activation_function_class.function(np.dot(matrix_hidden_units, self.W) + self.b[np.newaxis, :])\n",
        "\n",
        "    def _compute_free_energy(self, vector_visible_units):\n",
        "        \"\"\"\n",
        "        Computes the RBM free energy.\n",
        "        :param vector_visible_units: array-like, shape = (n_features, )\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        v = vector_visible_units\n",
        "        return - np.dot(self.b, v) - np.sum(np.log(1 + np.exp(np.dot(self.W, v) + self.c)))\n",
        "\n",
        "    def _compute_reconstruction_error(self, data):\n",
        "        \"\"\"\n",
        "        Computes the reconstruction error of the data.\n",
        "        :param data: array-like, shape = (n_samples, n_features)\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        data_transformed = self.transform(data)\n",
        "        data_reconstructed = self._reconstruct(data_transformed)\n",
        "        return np.mean(np.sum((data_reconstructed - data) ** 2, 1))"
      ],
      "id": "888e9dcc",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e9c895c"
      },
      "source": [
        "class UnsupervisedDBN:\n",
        "    \"\"\"\n",
        "    This class implements a unsupervised Deep Belief Network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 hidden_layers_structure=[100, 100],\n",
        "                 activation_function='sigmoid',\n",
        "                 optimization_algorithm='sgd',\n",
        "                 learning_rate_rbm=1e-3,\n",
        "                 n_epochs_rbm=10,\n",
        "                 contrastive_divergence_iter=1,\n",
        "                 batch_size=32,\n",
        "                 verbose=True):\n",
        "        self.hidden_layers_structure = hidden_layers_structure\n",
        "        self.activation_function = activation_function\n",
        "        self.optimization_algorithm = optimization_algorithm\n",
        "        self.learning_rate_rbm = learning_rate_rbm\n",
        "        self.n_epochs_rbm = n_epochs_rbm\n",
        "        self.contrastive_divergence_iter = contrastive_divergence_iter\n",
        "        self.batch_size = batch_size\n",
        "        self.rbm_layers = None\n",
        "        self.verbose = verbose\n",
        "        self.rbm_class = BinaryRBM\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fits a model given data.\n",
        "        :param X: array-like, shape = (n_samples, n_features)\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # Initialize rbm layers\n",
        "        self.rbm_layers = list()\n",
        "        for n_hidden_units in self.hidden_layers_structure:\n",
        "            rbm = self.rbm_class(n_hidden_units=n_hidden_units,\n",
        "                                 activation_function=self.activation_function,\n",
        "                                 optimization_algorithm=self.optimization_algorithm,\n",
        "                                 learning_rate=self.learning_rate_rbm,\n",
        "                                 n_epochs=self.n_epochs_rbm,\n",
        "                                 contrastive_divergence_iter=self.contrastive_divergence_iter,\n",
        "                                 batch_size=self.batch_size,\n",
        "                                 verbose=self.verbose)\n",
        "            self.rbm_layers.append(rbm)\n",
        "\n",
        "        # Fit RBM\n",
        "        if self.verbose:\n",
        "            print(\"[START] Pre-training step:\")\n",
        "        input_data = X\n",
        "        for rbm in self.rbm_layers:\n",
        "            rbm.fit(input_data)\n",
        "            input_data = rbm.transform(input_data)\n",
        "        if self.verbose:\n",
        "            print(\"[END] Pre-training step\")\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Transforms data using the fitted model.\n",
        "        :param X: array-like, shape = (n_samples, n_features)\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        input_data = X\n",
        "        for rbm in self.rbm_layers:\n",
        "            input_data = rbm.transform(input_data)\n",
        "        return input_data\n"
      ],
      "id": "3e9c895c",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db213183"
      },
      "source": [
        "class SigmoidActivationFunction:\n",
        "    @classmethod\n",
        "    def function(cls, x):\n",
        "        \"\"\"\n",
        "        Sigmoid function.\n",
        "        :param x: array-like, shape = (n_features, )\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        return expit(x)\n",
        "\n",
        "    @classmethod\n",
        "    def prime(cls, x):\n",
        "        \"\"\"\n",
        "        Compute sigmoid first derivative.\n",
        "        :param x: array-like, shape = (n_features, )\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        return x * (1 - x)"
      ],
      "id": "db213183",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "828dd9db"
      },
      "source": [
        "class ReLUActivationFunction:\n",
        "    @classmethod\n",
        "    def function(cls, x):\n",
        "        \"\"\"\n",
        "        Rectified linear function.\n",
        "        :param x: array-like, shape = (n_features, )\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        return np.maximum(np.zeros(x.shape), x)\n",
        "\n",
        "    @classmethod\n",
        "    def prime(cls, x):\n",
        "        \"\"\"\n",
        "        Rectified linear first derivative.\n",
        "        :param x: array-like, shape = (n_features, )\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        return (x > 0).astype(int)\n"
      ],
      "id": "828dd9db",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11e04c6c",
        "outputId": "c44e758a-bdc6-4a21-9510-bcb1500be23f"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    \n",
        "    X, y = load_dataset()\n",
        "    \n",
        "    prep = PreProcessing(X , y, pca_components = 10, window_size = 3)\n",
        "    \n",
        "    prep.spectrum_vector()\n",
        "    \n",
        "    prep.apply_pca()\n",
        "    \n",
        "    prep.spatial_vector()\n",
        "    \n",
        "    prep.remove_class_zero()\n",
        "    \n",
        "    #prep.combine_vectors()\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = prep.split_dataset() \n",
        "    \n",
        "    # Models we will use\n",
        "    logistic = linear_model.LogisticRegression(multi_class='multinomial')\n",
        "    #logistic = linear_model.LogisticRegression(multi_class='multinomial',solver='saga',class_weight='balanced', max_iter=10000, dual=False)\n",
        "    dbn = UnsupervisedDBN(hidden_layers_structure=[100, 100, 100],\n",
        "                          batch_size=10,\n",
        "                          learning_rate_rbm=0.01,\n",
        "                          n_epochs_rbm=50,\n",
        "                          activation_function='sigmoid')\n",
        "\n",
        "    classifier = Pipeline(steps=[('dbn', dbn),\n",
        "                             ('logistic', logistic)])\n",
        "\n",
        "\n",
        "    # Training\n",
        "    #logistic.C = 6000.0\n",
        "\n",
        "    # Training RBM-Logistic Pipeline\n",
        "    classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Training Logistic regression\n",
        "    logistic_classifier = linear_model.LogisticRegression(C=100.0)\n",
        "    logistic_classifier.fit(X_train, y_train)\n",
        "    \n",
        "    # Evaluation\n",
        "\n",
        "    print()\n",
        "    print(\"Logistic regression using RBM features:\\n%s\\n\" % (classification_report(y_test,\n",
        "                                                                                   classifier.predict(X_test))))\n",
        "\n",
        "    print(\"Logistic regression using raw pixel features:\\n%s\\n\" % (classification_report(y_test,\n",
        "                                                                                         logistic_classifier.predict(X_test))))\n",
        "     # Evaluation\n",
        "\n",
        "   "
      ],
      "id": "11e04c6c",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load dataset\n",
            "\n",
            "X shape : (610, 340, 103)\n",
            "y shape : (610, 340)\n",
            "\n",
            "\n",
            "\n",
            "Spectrum vector shape : (207400, 103)\n",
            "\n",
            "\n",
            "Apply PCA\n",
            "\n",
            "temp_X shape : (207400, 103)\n",
            "X shape after applying PCA dimensionality reduction technique : (610, 340, 10)\n",
            "\n",
            "\n",
            "\n",
            "Create smaller image cuboids\n",
            "\n",
            "The shape of X after zero padding : (612, 342, 10)\n",
            "The shape of y after zero padding : (612, 342)\n",
            "\n",
            "\n",
            "Spatial vector shape : (207400, 90)\n",
            "The shape of y_cuboids_labels : (207400,)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Removing class zero\n",
            "\n",
            "Classes available for classification after removing class 0 : [1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
            "New Class labels starting from 0 :  [0. 1. 2. 3. 4. 5. 6. 7. 8.]\n",
            "\n",
            "\n",
            "Spectrum vector shape after removing class zero: (42776, 103)\n",
            "\n",
            "\n",
            "Spatial vector shape after removing class zero: (42776, 90)\n",
            "The shape of y after removing class zero : (42776,)\n",
            "\n",
            "\n",
            "\n",
            "Combined final spactial vector shape : (42776, 90)\n",
            "y shape : (42776,)\n",
            "\n",
            "\n",
            "Splitting Dataset\n",
            "\n",
            "Number of elements present in each class of y: {0.0: 6631, 1.0: 18649, 2.0: 2099, 3.0: 3064, 4.0: 1345, 5.0: 5029, 6.0: 1330, 7.0: 3682, 8.0: 947}\n",
            "X_train shape : (25665, 90)\n",
            "y_train shape : (25665,)\n",
            "X_test shape  : (17111, 90)\n",
            "y_test shape  : (17111,)\n",
            "Number of elements present in each class of y_train: {0.0: 3979, 1.0: 11189, 2.0: 1260, 3.0: 1838, 4.0: 807, 5.0: 3017, 6.0: 798, 7.0: 2209, 8.0: 568}\n",
            "Number of elements present in each class of y_test: {0.0: 2652, 1.0: 7460, 2.0: 839, 3.0: 1226, 4.0: 538, 5.0: 2012, 6.0: 532, 7.0: 1473, 8.0: 379}\n",
            "[START] Pre-training step:\n",
            ">> Epoch 1 finished \tRBM Reconstruction error 72.341696\n",
            ">> Epoch 2 finished \tRBM Reconstruction error 72.928989\n",
            ">> Epoch 3 finished \tRBM Reconstruction error 73.088508\n",
            ">> Epoch 4 finished \tRBM Reconstruction error 73.211258\n",
            ">> Epoch 5 finished \tRBM Reconstruction error 73.398718\n",
            ">> Epoch 6 finished \tRBM Reconstruction error 73.381358\n",
            ">> Epoch 7 finished \tRBM Reconstruction error 73.563219\n",
            ">> Epoch 8 finished \tRBM Reconstruction error 73.450120\n",
            ">> Epoch 9 finished \tRBM Reconstruction error 73.378849\n",
            ">> Epoch 10 finished \tRBM Reconstruction error 73.324937\n",
            ">> Epoch 11 finished \tRBM Reconstruction error 73.320935\n",
            ">> Epoch 12 finished \tRBM Reconstruction error 73.462707\n",
            ">> Epoch 13 finished \tRBM Reconstruction error 73.723619\n",
            ">> Epoch 14 finished \tRBM Reconstruction error 73.820455\n",
            ">> Epoch 15 finished \tRBM Reconstruction error 73.815195\n",
            ">> Epoch 16 finished \tRBM Reconstruction error 73.612658\n",
            ">> Epoch 17 finished \tRBM Reconstruction error 73.601348\n",
            ">> Epoch 18 finished \tRBM Reconstruction error 73.576510\n",
            ">> Epoch 19 finished \tRBM Reconstruction error 73.765854\n",
            ">> Epoch 20 finished \tRBM Reconstruction error 73.544580\n",
            ">> Epoch 21 finished \tRBM Reconstruction error 73.521726\n",
            ">> Epoch 22 finished \tRBM Reconstruction error 73.621244\n",
            ">> Epoch 23 finished \tRBM Reconstruction error 73.691659\n",
            ">> Epoch 24 finished \tRBM Reconstruction error 73.444097\n",
            ">> Epoch 25 finished \tRBM Reconstruction error 73.972823\n",
            ">> Epoch 26 finished \tRBM Reconstruction error 73.971855\n",
            ">> Epoch 27 finished \tRBM Reconstruction error 73.689096\n",
            ">> Epoch 28 finished \tRBM Reconstruction error 74.116095\n",
            ">> Epoch 29 finished \tRBM Reconstruction error 73.663670\n",
            ">> Epoch 30 finished \tRBM Reconstruction error 73.848460\n",
            ">> Epoch 31 finished \tRBM Reconstruction error 73.664749\n",
            ">> Epoch 32 finished \tRBM Reconstruction error 74.116239\n",
            ">> Epoch 33 finished \tRBM Reconstruction error 73.998038\n",
            ">> Epoch 34 finished \tRBM Reconstruction error 73.626535\n",
            ">> Epoch 35 finished \tRBM Reconstruction error 73.984963\n",
            ">> Epoch 36 finished \tRBM Reconstruction error 73.791422\n",
            ">> Epoch 37 finished \tRBM Reconstruction error 74.177815\n",
            ">> Epoch 38 finished \tRBM Reconstruction error 74.266556\n",
            ">> Epoch 39 finished \tRBM Reconstruction error 73.811316\n",
            ">> Epoch 40 finished \tRBM Reconstruction error 73.910020\n",
            ">> Epoch 41 finished \tRBM Reconstruction error 73.891030\n",
            ">> Epoch 42 finished \tRBM Reconstruction error 74.239688\n",
            ">> Epoch 43 finished \tRBM Reconstruction error 74.168527\n",
            ">> Epoch 44 finished \tRBM Reconstruction error 74.072345\n",
            ">> Epoch 45 finished \tRBM Reconstruction error 73.825758\n",
            ">> Epoch 46 finished \tRBM Reconstruction error 73.941411\n",
            ">> Epoch 47 finished \tRBM Reconstruction error 74.278653\n",
            ">> Epoch 48 finished \tRBM Reconstruction error 74.277863\n",
            ">> Epoch 49 finished \tRBM Reconstruction error 73.943555\n",
            ">> Epoch 50 finished \tRBM Reconstruction error 74.113128\n",
            ">> Epoch 1 finished \tRBM Reconstruction error 2.180851\n",
            ">> Epoch 2 finished \tRBM Reconstruction error 1.469463\n",
            ">> Epoch 3 finished \tRBM Reconstruction error 1.170708\n",
            ">> Epoch 4 finished \tRBM Reconstruction error 0.977997\n",
            ">> Epoch 5 finished \tRBM Reconstruction error 0.854756\n",
            ">> Epoch 6 finished \tRBM Reconstruction error 0.774773\n",
            ">> Epoch 7 finished \tRBM Reconstruction error 0.695295\n",
            ">> Epoch 8 finished \tRBM Reconstruction error 0.642881\n",
            ">> Epoch 9 finished \tRBM Reconstruction error 0.589642\n",
            ">> Epoch 10 finished \tRBM Reconstruction error 0.556397\n",
            ">> Epoch 11 finished \tRBM Reconstruction error 0.522145\n",
            ">> Epoch 12 finished \tRBM Reconstruction error 0.496582\n",
            ">> Epoch 13 finished \tRBM Reconstruction error 0.466457\n",
            ">> Epoch 14 finished \tRBM Reconstruction error 0.445080\n",
            ">> Epoch 15 finished \tRBM Reconstruction error 0.429111\n",
            ">> Epoch 16 finished \tRBM Reconstruction error 0.407384\n",
            ">> Epoch 17 finished \tRBM Reconstruction error 0.393389\n",
            ">> Epoch 18 finished \tRBM Reconstruction error 0.374948\n",
            ">> Epoch 19 finished \tRBM Reconstruction error 0.362637\n",
            ">> Epoch 20 finished \tRBM Reconstruction error 0.348450\n",
            ">> Epoch 21 finished \tRBM Reconstruction error 0.338111\n",
            ">> Epoch 22 finished \tRBM Reconstruction error 0.325476\n",
            ">> Epoch 23 finished \tRBM Reconstruction error 0.318350\n",
            ">> Epoch 24 finished \tRBM Reconstruction error 0.308907\n",
            ">> Epoch 25 finished \tRBM Reconstruction error 0.302281\n",
            ">> Epoch 26 finished \tRBM Reconstruction error 0.294561\n",
            ">> Epoch 27 finished \tRBM Reconstruction error 0.286618\n",
            ">> Epoch 28 finished \tRBM Reconstruction error 0.279880\n",
            ">> Epoch 29 finished \tRBM Reconstruction error 0.272410\n",
            ">> Epoch 30 finished \tRBM Reconstruction error 0.265445\n",
            ">> Epoch 31 finished \tRBM Reconstruction error 0.260421\n",
            ">> Epoch 32 finished \tRBM Reconstruction error 0.255736\n",
            ">> Epoch 33 finished \tRBM Reconstruction error 0.250434\n",
            ">> Epoch 34 finished \tRBM Reconstruction error 0.244699\n",
            ">> Epoch 35 finished \tRBM Reconstruction error 0.240309\n",
            ">> Epoch 36 finished \tRBM Reconstruction error 0.234126\n",
            ">> Epoch 37 finished \tRBM Reconstruction error 0.228805\n",
            ">> Epoch 38 finished \tRBM Reconstruction error 0.224623\n",
            ">> Epoch 39 finished \tRBM Reconstruction error 0.220960\n",
            ">> Epoch 40 finished \tRBM Reconstruction error 0.216435\n",
            ">> Epoch 41 finished \tRBM Reconstruction error 0.211059\n",
            ">> Epoch 42 finished \tRBM Reconstruction error 0.207025\n",
            ">> Epoch 43 finished \tRBM Reconstruction error 0.204166\n",
            ">> Epoch 44 finished \tRBM Reconstruction error 0.201012\n",
            ">> Epoch 45 finished \tRBM Reconstruction error 0.198644\n",
            ">> Epoch 46 finished \tRBM Reconstruction error 0.194025\n",
            ">> Epoch 47 finished \tRBM Reconstruction error 0.191051\n",
            ">> Epoch 48 finished \tRBM Reconstruction error 0.188260\n",
            ">> Epoch 49 finished \tRBM Reconstruction error 0.184806\n",
            ">> Epoch 50 finished \tRBM Reconstruction error 0.181186\n",
            ">> Epoch 1 finished \tRBM Reconstruction error 1.468365\n",
            ">> Epoch 2 finished \tRBM Reconstruction error 0.789352\n",
            ">> Epoch 3 finished \tRBM Reconstruction error 0.535871\n",
            ">> Epoch 4 finished \tRBM Reconstruction error 0.408501\n",
            ">> Epoch 5 finished \tRBM Reconstruction error 0.333257\n",
            ">> Epoch 6 finished \tRBM Reconstruction error 0.280045\n",
            ">> Epoch 7 finished \tRBM Reconstruction error 0.247477\n",
            ">> Epoch 8 finished \tRBM Reconstruction error 0.217476\n",
            ">> Epoch 9 finished \tRBM Reconstruction error 0.197488\n",
            ">> Epoch 10 finished \tRBM Reconstruction error 0.180482\n",
            ">> Epoch 11 finished \tRBM Reconstruction error 0.166696\n",
            ">> Epoch 12 finished \tRBM Reconstruction error 0.153416\n",
            ">> Epoch 13 finished \tRBM Reconstruction error 0.143571\n",
            ">> Epoch 14 finished \tRBM Reconstruction error 0.134454\n",
            ">> Epoch 15 finished \tRBM Reconstruction error 0.127218\n",
            ">> Epoch 16 finished \tRBM Reconstruction error 0.120820\n",
            ">> Epoch 17 finished \tRBM Reconstruction error 0.114609\n",
            ">> Epoch 18 finished \tRBM Reconstruction error 0.109162\n",
            ">> Epoch 19 finished \tRBM Reconstruction error 0.105171\n",
            ">> Epoch 20 finished \tRBM Reconstruction error 0.100473\n",
            ">> Epoch 21 finished \tRBM Reconstruction error 0.096727\n",
            ">> Epoch 22 finished \tRBM Reconstruction error 0.094733\n",
            ">> Epoch 23 finished \tRBM Reconstruction error 0.091613\n",
            ">> Epoch 24 finished \tRBM Reconstruction error 0.089736\n",
            ">> Epoch 25 finished \tRBM Reconstruction error 0.086832\n",
            ">> Epoch 26 finished \tRBM Reconstruction error 0.084260\n",
            ">> Epoch 27 finished \tRBM Reconstruction error 0.081449\n",
            ">> Epoch 28 finished \tRBM Reconstruction error 0.080702\n",
            ">> Epoch 29 finished \tRBM Reconstruction error 0.078918\n",
            ">> Epoch 30 finished \tRBM Reconstruction error 0.076315\n",
            ">> Epoch 31 finished \tRBM Reconstruction error 0.074989\n",
            ">> Epoch 32 finished \tRBM Reconstruction error 0.073764\n",
            ">> Epoch 33 finished \tRBM Reconstruction error 0.072366\n",
            ">> Epoch 34 finished \tRBM Reconstruction error 0.070996\n",
            ">> Epoch 35 finished \tRBM Reconstruction error 0.068868\n",
            ">> Epoch 36 finished \tRBM Reconstruction error 0.068967\n",
            ">> Epoch 37 finished \tRBM Reconstruction error 0.068099\n",
            ">> Epoch 38 finished \tRBM Reconstruction error 0.066116\n",
            ">> Epoch 39 finished \tRBM Reconstruction error 0.065674\n",
            ">> Epoch 40 finished \tRBM Reconstruction error 0.064700\n",
            ">> Epoch 41 finished \tRBM Reconstruction error 0.064107\n",
            ">> Epoch 42 finished \tRBM Reconstruction error 0.062810\n",
            ">> Epoch 43 finished \tRBM Reconstruction error 0.062939\n",
            ">> Epoch 44 finished \tRBM Reconstruction error 0.061439\n",
            ">> Epoch 45 finished \tRBM Reconstruction error 0.061162\n",
            ">> Epoch 46 finished \tRBM Reconstruction error 0.059386\n",
            ">> Epoch 47 finished \tRBM Reconstruction error 0.059474\n",
            ">> Epoch 48 finished \tRBM Reconstruction error 0.058634\n",
            ">> Epoch 49 finished \tRBM Reconstruction error 0.057944\n",
            ">> Epoch 50 finished \tRBM Reconstruction error 0.057264\n",
            "[END] Pre-training step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Logistic regression using RBM features:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.72      0.89      0.80      2652\n",
            "         1.0       0.85      0.95      0.90      7460\n",
            "         2.0       0.68      0.39      0.50       839\n",
            "         3.0       0.94      0.85      0.89      1226\n",
            "         4.0       0.99      0.99      0.99       538\n",
            "         5.0       0.76      0.45      0.56      2012\n",
            "         6.0       0.43      0.28      0.34       532\n",
            "         7.0       0.66      0.80      0.72      1473\n",
            "         8.0       0.00      0.00      0.00       379\n",
            "\n",
            "    accuracy                           0.80     17111\n",
            "   macro avg       0.67      0.62      0.63     17111\n",
            "weighted avg       0.77      0.80      0.77     17111\n",
            "\n",
            "\n",
            "Logistic regression using raw pixel features:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.97      0.96      2652\n",
            "         1.0       0.94      0.97      0.96      7460\n",
            "         2.0       0.94      0.88      0.91       839\n",
            "         3.0       0.95      0.93      0.94      1226\n",
            "         4.0       1.00      1.00      1.00       538\n",
            "         5.0       0.88      0.81      0.85      2012\n",
            "         6.0       0.92      0.87      0.89       532\n",
            "         7.0       0.91      0.95      0.93      1473\n",
            "         8.0       1.00      0.99      0.99       379\n",
            "\n",
            "    accuracy                           0.94     17111\n",
            "   macro avg       0.94      0.93      0.94     17111\n",
            "weighted avg       0.94      0.94      0.94     17111\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3ZA9_vEUvsm"
      },
      "source": [
        ""
      ],
      "id": "K3ZA9_vEUvsm",
      "execution_count": null,
      "outputs": []
    }
  ]
}